---
title: "Literary Review"
author: "Travis Keep (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

While normal linear regression has many useful applications, it does not accurately explain all models. For instance, normal linear regression does not always provide good models where the dependent variable has a definite minimum and maximum. Test scores are a good example, where scores typically fall between 0 and 100. In this paper, we discuss a new method, beta regression, and how it often does a better job of explaining data with a definite minimum and maximum, and we discuss how beta regression can be used in R.

When using conventional linear regression on a dependent variable with values ranging between 0 and 1, we may apply the logit function to the dependent variable and then fit a linear model to that. The logit function is

$$
logit(p) = log \left(\frac{p}{1-p}\right)
$$
The logit function transforms the input value, which is between 0 and 1, to all the real values. The result of the logit function can be anywhere between $-\infty$ and $+\infty$. Doing this with regular regression, does not always provide an accurate model. For example, consider 44 reading scores based on the child's IQ and whether or not the child is dyslexic. These reading scores were scaled so that they fall between 0 and 1. Using normal regression showed that IQ alone does not affect reading score which is not actually the case [@smithson2006]. However when this same reading scores data is modeled with beta regression, IQ alone does have a significant effect on test scores. That is, the higher the IQ the higher the reading test score regardless of whether or not the child is dyslexic [@smithson2006].

Beta regression aims to create more accurate models when the dependent variable falls within a certain range such as between 0 and 1. Beta regression assumes that at a given point, the dependent variable follows a beta distribution instead of a normal distribution [@grun2012].

Data which are percentages typically follow beta distributions for example test scores. Classical beta distributions have two shape parameters, a and b. The probability density function of a beta distribution is:

$$
f(y; a, b) = \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}y^{a-1}(1 - y)^{b-1}
$$
Note that y falls in the interval (0, 1) [@cribari2002]. The mean and variance of a beta distribution are:

$$
\mu = \frac{a}{a+b} \quad \sigma^2 = \frac{ab}{(a+b)^2(a+b+1)}
$$
[@ferrari2004]. For our application of beta distributions, it is useful to rewrite the beta distribution shapes in terms of $\mu$ and $\phi$ where $\mu$ is the mean and $\phi$ controls the variance. The larger the $\phi$, the less the variance. [@ferrari2004]. In particular

$$
\phi = a + b \quad \mu = \frac{a}{a + b}
$$
So variance becomes

$$
\sigma^2 = \frac{\mu(1 - \mu)}{1 + \phi}
$$
[@ferrari2004].

Beta regression assumes that the dependent variable follows a beta distribution. Like in standard linear regression, we apply the logit function to the dependent variable and fit a linear model to that. Unlike linear regression, we also apply the natural logarithm function to phi and fit a linear model to that. [@smithson2006]. So we have

$$
logit(\mu_i) = x_i\beta
$$
$$
ln(\phi_i) = -w_i\delta
$$
[@smithson2006]. Note that $w_i$ is often some subset of $x_i$. The minus sign is in the second equation so that larger coefficients in $\delta$ mean a larger variance. [@smithson2006]. We find the coefficients of $\beta$ and $\delta$ by maximizing the log likelihood function which is in terms of $\beta$ and $\delta$.

Note that $\phi$ can be modeled as being constant throughout or it can be modeled as being dependent on certain regressors [@smithson2006].

With $\beta$ we can predict $\mu$ as follows: 

$$
\mu = \frac{e^{x \beta}}{e^{x \beta} + 1}
$$
Like in linear regression, the $\beta_i$ have a definite interpretation. If $x_i$ increases by 1, the odds of $\mu$ multiply by $e^{b_i}$. Where the odds of $\mu$ are $\frac{\mu}{1-\mu}$ [@ferrari2004].

In conclusion, because beta regression assumes dependent variables have a beta distribution instead of a normal distribution, beta regression provides superior models to normal regression when the dependent variable is a percentage or has a definite minimum and maximum value.


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

## References
