---
title: "Final Paper"
author: "Travis Keep (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
csl: ieee.csl
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

While normal linear regression has many useful applications, it does not accurately explain all models. For instance, normal linear regression does not always provide good models where the dependent variable has a definite minimum and maximum. Test scores are a good example, where scores typically fall between 0 and 100. In this paper, we discuss a new method, beta regression, and how it often does a better job of explaining data with a definite minimum and maximum, and we discuss how beta regression can be used in R.

## General Linear Regression

First we discuss general linear regression. In general linear regression, the dependent variable is modeled as a linear combination of the regressors in the model plus some constant. For example $\hat{y}=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \cdots$. The $\beta_i$'s are chosen to minimise the the Sum of Squares error which is

$$
SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
The interpretation of the $\beta_i$ is as follows. If the regressosr $x_i$ increases by 1, then $\hat{y}$ increases by $\beta_i$. [@kutner2004].

An ANOVA table can test the significance of a general linear regression model.

$$
\begin{align}
SST &= \sum_{i=1}^{n} (y_i - \bar{y})^2 \\
SSR &= \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2 \\
SSE &= \sum_{i=1}^{n} (y_i - \hat{y_i})^2
\end{align}
$$
Where SST is the total variance of Y; SSR is the variance explained by the model. SSE is the variance not explained by the model. [@kutner2004].

ANOVA table

| Sum of Squares | degrees of freedom | Mean Square | F |
| -------------- | ------------------ | ----------- | - |
| SSR | p | $MSR=\frac{SSR}{p}$ | |
| SSE | n - p - 1 | $MSE=\frac{SSE}{n-p-1}$ | $\frac{MSR}{MSE}$ |
| SST | n - 1 | | |

$p$ is the number of betas excluding $\beta_0$.

if $F > F_\alpha(p, n-p-1)$ then the model is significant. [@kutner2004].

## Why General Linear Regression is not Always Suitable

General linear regression assumes that each $y_i$ follows a normal distribution with mean $\hat{y_i}$ and constant variance. When the dependent variable has a definite minumum and maximum such as a ratio or percentage, this assumption that general linear regression makes does not hold. The variance of the $y_i$'s is not constant, but becomes less when $\hat{y_i}$ is close to the minimum or maximum. Sometimes we must assume that each $y_i$ follows a beta distribution instead of a normal distribution.

## Beta Distribution

The beta distribution is for a random variable that falls on the open set (0, 1). Note that it is possible to scale any random variable with a definite minimum and maximum to this interval. Below is the PDF for the beta distribution.

$$
f(y) = \begin{cases}
      \frac{y^{\alpha-1}(1-y)^{\beta-1}}{B(\alpha,\beta)}, & 0 \le y \le 1 \\
      0, & \text{elsewhere}
\end{cases}
$$
Where $B(\alpha,\beta) = \int_0^1 y^{\alpha-1}(1-y)^{\beta-1} \ dy = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}.$

$\alpha$ and $\beta$ are the shape variables where $\alpha > 0 \quad \beta > 0$.
[@wackerly2002]

$$
E[Y] = \mu = \frac{\alpha}{\alpha+\beta} \ \ \ \text{and} \ \ \ V[Y] = \sigma^2  = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
$$
[@wackerly2002]

For beta regression it is useful to introduce the following,

$$
\begin{align}
\mu &= \frac{\alpha}{\alpha+\beta} \\
\phi &= \alpha + \beta
\end{align}
$$
$\mu$ is the mean of the beta regression while the higher the $\phi$ the less the variance or the less spread out the PDF function is. [@ferrari2004]

We can rewrite the beta distribution PDF function in terms of $\mu$ and $\phi$.

$$
f(y; \mu, \phi) = \frac{\Gamma(\phi)}{\Gamma(\mu \phi) \Gamma((1 - \mu)\phi)} y^{\mu\phi - 1}(1 - y)^{(1 - \mu)\phi - 1}, \quad 0 < y < 1
$$
Where:\
- **(μ)** is the mean, **(ϕ)** is the precision (inverse of the variance), **(Γ)** is the gamma function.

$$
\text{Var}(Y) = \frac{\mu(1 - \mu)}{1 + \phi}
$$
When $\mu$ is near the extremes, 0 or 1, variance drops. [@ferrari2004]

## Beta Regression

With beta regression, we first scale the dependent variable so that it falls on the (0, 1) interval. We then apply the logit function to the dependent variable.

$$
logit(p) = log \left(\frac{p}{1-p}\right)
$$
The logit function transforms the input value, which is between 0 and 1, to all the real values. The result of the logit function can be anywhere between $-\infty$ and $+\infty$.

Beta regression aims to create more accurate models when the dependent variable falls within a certain range such as between 0 and 1. Beta regression assumes that at a given point, the dependent variable follows a beta distribution instead of a normal distribution [@grun2012].

We apply the logit function to the dependent variable, $\mu_i$, and fit a linear model to that. Unlike linear regression, we also apply the natural logarithm function to phi and fit a linear model to that. [@smithson2006]. So we have

$$
logit(\mu_i) = x_i\beta
$$
$$
ln(\phi_i) = -w_i\delta
$$

[@smithson2006]. Note that $w_i$ is often some subset of $x_i$. The minus sign is in the second equation so that larger coefficients in $\delta$ mean a larger variance. [@smithson2006]. We find the coefficients of $\beta$ and $\delta$ by maximizing the log likelihood function which is in terms of $\beta$ and $\delta$.  R supports three methods for finding the $\beta_i$ and $\delta_i$, ML which is just standard maximum likelihood. BC, which is maximum likelihood with bias correction, and BR which is maximum likelihood with bias reduction [@grun2012].

Note that $\phi$ can be modeled as being constant throughout or it can be modeled as being dependent on certain regressors [@smithson2006].

With $\beta$ we can predict $\mu$ as follows:

$$
\mu = \frac{e^{x \beta}}{e^{x \beta} + 1}
$$

Like in linear regression, the $\beta_i$ have a definite interpretation. If $x_i$ increases by 1 with all other regressors remaining the same, the odds of $\mu$ multiply by $e^{b_i}$. Where the odds of $\mu$ are $\frac{\mu}{1-\mu}$ [@ferrari2004].

## The Reading Skills Dataset

This is the ReadingSkills dataset included with the R betareg package. This dataset explores reading skills for 44 school aged children. It is significant because it shows that beta regression shows that IQ alone is significant in predicting reading scores while regular regression techniques show that IQ alone is not significant in predicting reading scores.

Here are the description of the columns

| Name | Type | Description |
|------|------|-------------|
| accuracy | numeric | Reading score with max of 0.99 |
| dyslexia | character | Yes for dyslexic, No for not dyslexic |
| iq | numeric | Non verbal intelligence quotient transform to z score |
| accuracy1 | numeric | same as accuracy but score goes up to 1.0 |



```{r, warning=FALSE, echo=T, message=FALSE}
library(betareg)
library(dplyr)
library(magrittr)
data("ReadingSkills")

dyslexia <- ReadingSkills %>% filter(dyslexia == "yes")
normal <- ReadingSkills %>% filter(dyslexia == "no")
n <- (ReadingSkills %>% dim)[1]
ndyslexia <- (dyslexia %>% dim)[1]
nnormal <- (normal %>% dim)[1]
iq_stats <- ReadingSkills %>% summarize(min(iq), median(iq), max(iq))
iq_dyslexia_stats <- dyslexia %>% summarize(min(iq), median(iq), max(iq))
iq_normal_stats <- normal %>% summarize(min(iq), median(iq), max(iq))
score_stats <- ReadingSkills %>% summarize(min(accuracy1), median(accuracy1), max(accuracy1))
score_dyslexia_stats <- dyslexia %>% summarize(min(accuracy1), median(accuracy1), max(accuracy1))
score_normal_stats <- normal %>% summarize(min(accuracy1), median(accuracy1), max(accuracy1))
```

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-kg9c{background-color:#D9D9D9;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0thz{border-color:inherit;font-weight:bold;text-align:left;vertical-align:bottom}
.tg .tg-j6zm{font-weight:bold;text-align:left;vertical-align:bottom}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}
.tg .tg-7zrl{text-align:left;vertical-align:bottom}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-kg9c" colspan="5" rowspan="2">Table 1: Student ReadingSkills data stratified by dyslexia</th>
  </tr>
  <tr>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0thz" colspan="2"><span style="font-weight:bold">Student Characteristic</span></td>
    <td class="tg-0thz"><span style="font-weight:bold">Total (n=`r n`)</span></td>
    <td class="tg-0thz"><span style="font-weight:bold">Dyslexia (n=`r ndyslexia`)</span></td>
    <td class="tg-j6zm"><span style="font-weight:bold">Normal (n=`r nnormal`)</span></td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="3">IQ (Z-Score)</td>
    <td class="tg-0pky">min</td>
    <td class="tg-za14">`r iq_stats[1, 1]`</td>
    <td class="tg-za14">`r iq_dyslexia_stats[1, 1]`</td>
    <td class="tg-za14">`r iq_normal_stats[1, 1]`</td>
  </tr>
  <tr>
    <td class="tg-0pky">median</td>
    <td class="tg-za14">`r iq_stats[1, 2]`</td>
    <td class="tg-za14">`r iq_dyslexia_stats[1, 2]`</td>
    <td class="tg-za14">`r iq_normal_stats[1, 2]`</td>
  </tr>
  <tr>
    <td class="tg-0pky">max</td>
    <td class="tg-za14">`r iq_stats[1, 3]`</td>
    <td class="tg-za14">`r iq_dyslexia_stats[1, 3]`</td>
    <td class="tg-za14">`r iq_normal_stats[1, 3]`</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="3">Reading Score</td>
    <td class="tg-0pky">min</td>
    <td class="tg-za14">`r score_stats[1, 1]`</td>
    <td class="tg-za14">`r score_dyslexia_stats[1, 1]`</td>
    <td class="tg-za14">`r score_normal_stats[1, 1]`</td>
  </tr>
  <tr>
    <td class="tg-0pky">median</td>
    <td class="tg-za14">`r score_stats[1, 2]`</td>
    <td class="tg-za14">`r score_dyslexia_stats[1, 2]`</td>
    <td class="tg-za14">`r score_normal_stats[1, 2]`</td>
  </tr>
  <tr>
    <td class="tg-0pky">max</td>
    <td class="tg-za14">`r score_stats[1, 3]`</td>
    <td class="tg-za14">`r score_dyslexia_stats[1, 3]`</td>
    <td class="tg-za14">`r score_normal_stats[1, 3]`</td>
  </tr>
</tbody></table>


```{r, warning=F, echo=T, message=F}
library(ggplot2)
coords <- aes(x=iq, y=accuracy1)
ggplot(ReadingSkills, coords) + geom_point(aes(color=dyslexia)) + labs(x="IQ (Z-score)", y="score") + ggtitle("Score by IQ for all students")
```

## Analysis

In this section, we do beta regression analysis on the ReadingSkills data set. First, we add a numeric equivalent for dyslexia, dcode, to our dataset. For “no”, we code 0.0; for “yes” we code 1.0. We use the full model for $\mu$ (dcode, iq, and dcode:iq), but we model $\phi$ with just dcode and iq and no interaction term. Modeling phi in this way makes IQ and the interaction term significant. We use "BC" (Bias Correction) which is the same as maximum likelihood except that it corrects for bias. To visualise the regression, we split the results into dyslexic children and normal children [@francisco2010].

In addition to the beta regression, we also do general linear regression using the logit link function. logit has the special property of transforming test scores falling on the interval (0, 1) to values falling on the set $\mathbb{R}$. When predicting, test scores with this model, we apply the inverse logit function to map from the $\mathbb{R}$ back to the interval (0, 1) [@R-base].

$$
\begin{align}
logit(p) &= log \left( \frac{p}{1-p} \right) \\
invlogit(p) &= \frac{e^p}{e^p + 1}
\end{align}
$$

```{r, warning=F, echo=T, message=F}

ReadingSkillsModel <- ReadingSkills %>% mutate(dcode=ifelse(dyslexia == "no", 0.0, 1.0))

model <- betareg(accuracy ~ dcode*iq | dcode + iq, data=ReadingSkillsModel, type="BC")
linmodel <- glm(accuracy ~ dcode*iq, family=gaussian(link="logit"), data=ReadingSkillsModel)

dyslexiaModel <- ReadingSkillsModel %>% filter(dyslexia == "yes")
normalModel <- ReadingSkillsModel %>% filter(dyslexia == "no")

ggplot() + geom_point(aes(x=normalModel$iq, y=normalModel$accuracy), color="red") +
geom_line(aes(x=normalModel$iq, y=predict(model, newdata=normalModel), color="beta")) +
geom_line(aes(x=normalModel$iq, y=plogis(predict(linmodel, newdata=normalModel)), color="glm")) +
xlab("IQ (Z-score)") + ylab("Score") + ggtitle("Score by IQ for normal students") +
scale_color_manual(name="Regression Model", breaks=c("beta", "glm"), values=c("beta"="red", "glm"="green"))

ggplot() + geom_point(aes(x=dyslexiaModel$iq, y=dyslexiaModel$accuracy), color="blue") +
geom_line(aes(x=dyslexiaModel$iq, y=predict(model, newdata=dyslexiaModel), color="beta")) +
geom_line(aes(x=dyslexiaModel$iq, y=plogis(predict(linmodel, newdata=dyslexiaModel)), color="glm")) +
xlab("IQ (Z-score)") + ylab("Score") + ggtitle("Score by IQ for dyslexic students") +
scale_color_manual(name="Regression Model", breaks=c("beta", "glm"), values=c("beta"="blue", "glm"="green"))

```

## Results

```{r, warning=F, echo=T, message=F}

model.summary <- summary(model)
linmodel.summary <- summary(linmodel)

model.coefs <- model.summary$coefficients$mean
linmodel.coefs <- linmodel.summary$coefficients

```

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-pb0m{border-color:inherit;text-align:center;vertical-align:bottom}
.tg .tg-kg9c{background-color:#D9D9D9;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}
.tg .tg-jkyp{border-color:inherit;text-align:right;vertical-align:bottom}
</style>
<table class="tg"><thead>
  <tr>
    <th class="tg-kg9c" colspan="7" rowspan="2">Table 2: Association of Reading Skills Score with IQ and presence of Dyslexia</th>
  </tr>
  <tr>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-c3ow" rowspan="2">Variable</td>
    <td class="tg-pb0m" colspan="3">Beta Regression</td>
    <td class="tg-pb0m" colspan="3">General Linear Regression</td>
  </tr>
  <tr>
    <td class="tg-pb0m">β</td>
    <td class="tg-pb0m">SE</td>
    <td class="tg-pb0m">p</td>
    <td class="tg-pb0m">β</td>
    <td class="tg-pb0m">SE</td>
    <td class="tg-pb0m">p</td>
  </tr>
  <tr>
    <td class="tg-za14">Dyslexia</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode', 4])`</td>
  </tr>
  <tr>
    <td class="tg-za14">IQ (Z-score)</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['iq', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['iq', 4])`</td>
  </tr>
  <tr>
    <td class="tg-za14">Dyslexia:iq</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', model.coefs['dcode:iq', 4])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 1])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 2])`</td>
    <td class="tg-jkyp">`r sprintf('%0.4g', linmodel.coefs['dcode:iq', 4])`</td>
  </tr>
</tbody></table>
<br>
<br>

In the beta regression model, dyslexia, IQ, and the interaction term are all significant. However, in the linear regression model, only dyslexia is significant. Both models show that the presence of dyslexia hurts reading scores. In the beta regression model, the beta value of -1.446 means that for children with an average IQ, if they are dyslexic their odds of answering a reading question correctly decreases by a factor of $e^{1.446}$. The beta value of 1.049 for IQ means that if a child's IQ increases by one standard deviation, then their odds of answering a reading question correctly increases by a factor of $e^{1.049}$. The interaction beta of 1.144 means that when dyslexia is present, if a child's IQ increases by one standard deviation, their odds of answering a reading question correctly decreases by a factor of $e^{0.095}$. This is because 1.049 - 1.144 = -0.095. The general linear regression model also shows that for normal children a higher IQ means a higher reading score. Like the beta regression model, the general linear regression model shows that when dyslexia is present that a higher IQ may mean slightly lower reading scores. The difference however is that for beta regression, all the betas are significant.

## Conclusion

In conclusion, because beta regression assumes dependent variables have a beta distribution instead of a normal distribution, beta regression can provide superior models to normal regression when the dependent variable is a percentage or has a definite minimum and maximum value. For the reading skills dataset, using beta regression shows that whether or not a child has dyslexia, the child's IQ, and the interaction between the two are all significant factors in predicting the reading test score. On the other hand, using regular linear regression with the logit link function only shows the presence of dyslexia as being significant. Beta regression shows that dyslexic children generally score lower on the reading test than normal children. For normal children, a higher IQ generally means a higher test score, but for dyslexic children, a higher IQ means a slightly lower test score. 


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

## References
